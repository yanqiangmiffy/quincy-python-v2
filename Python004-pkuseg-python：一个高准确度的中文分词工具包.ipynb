{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  1 简介\n",
    "\n",
    "> pkuseg-python简单易用，支持多领域分词，在不同领域的数据上都大幅提高了分词的准确率。\n",
    "\n",
    "pkuseg是由北京大学语言计算与机器学习研究组研制推出的一套全新的中文分词工具包。pkuseg具有如下几个特点：\n",
    "1. 高分词准确率。相比于其他的分词工具包，我们的工具包在不同领域的数据上都大幅提高了分词的准确度。根据我们的测试结果，pkuseg分别在示例数据集（MSRA和CTB8）上降低了79.33%和63.67%的分词错误率。\n",
    "2. 多领域分词。我们训练了多种不同领域的分词模型。根据待分词的领域特点，用户可以自由地选择不同的模型。\n",
    "3. 支持用户自训练模型。支持用户使用全新的标注数据进行训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 编译和安装\n",
    "1. 通过pip下载(自带模型文件)\n",
    "> pip install pkuseg\n",
    "> 之后通过import pkuseg来引用\n",
    "2. 从github下载(需要下载模型文件，见预训练模型)\n",
    "> 将pkuseg文件放到目录下，通过import pkuseg使用\n",
    "> 模型需要下载或自己训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 性能对比\n",
    "pkuseg官方给出与来自清华的THULAC和目前主流中文分词工具jieba的比较效果，pkuseg的准确率远超于THULAC和jieba。\n",
    "\n",
    "实验人员选择Linux作为测试环境，在新闻数据(MSRA)和混合型文本(CTB8)数据上对不同工具包进行了准确率测试，使用了第二届国际汉语分词评测比赛提供的分词评价脚本。评测结果如下：\n",
    "\n",
    "|MSRA | F-score| Error Rate |\n",
    "|:------------|------------:|------------:|\n",
    "| jieba |81.45 | 18.55\n",
    "| THULAC | 85.48 |  14.52\n",
    "| pkuseg | **96.75 (+13.18%)**| **3.25 (-77.62%)**\n",
    "\n",
    "\n",
    "|CTB8 | F-score | Error Rate|\n",
    "|:------------|------------:|------------:|\n",
    "|jieba|79.58|20.42\n",
    "|THULAC|87.77|12.23\n",
    "|pkuseg| **95.64 (+8.97%)**|**4.36 (-64.35%)**\n",
    "\n",
    "我们可以看到从F1-Score和错误率上，pkuseg都明显优于另外两者"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 使用教程\n",
    "### 代码实例1 使用默认模型及默认词典分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "finish\n",
      "['我', '爱', '北京', '天安门']\n"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "seg = pkuseg.pkuseg() #以默认配置加载模型\n",
    "text = seg.cut('我爱北京天安门') #进行分词\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例2 设置用户自定义词典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "finish\n",
      "['我', '爱', '北京天安门']\n"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "lexicon = ['北京大学', '北京天安门']\t#希望分词时用户词典中的词固定不分开\n",
    "seg = pkuseg.pkuseg(user_dict=lexicon)\t#加载模型，给定用户词典\n",
    "text = seg.cut('我爱北京天安门')\t\t#进行分词\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例3 指定模型\n",
    "默认情况下，`pkuseg`使用的预模型是`msra`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "finish\n",
      "['我', '爱', '北京', '天安门']\n"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "seg = pkuseg.pkuseg(model_name='ctb8') #假设用户已经下载好了ctb8的模型并放在了'./ctb8'目录下，通过设置model_name加载该模型\n",
    "text = seg.cut('我爱北京天安门') #进行分词\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例4 支持多线程\n",
    "\n",
    "可以指定输入和输出文件，直接从输入读取文本，将分词结果输出到结果文件"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading model\n",
      "finish\n",
      "Total time: 128.88791394233704\n"
     ]
    }
   ],
   "source": [
    "import pkuseg\n",
    "pkuseg.test('data/input.txt', 'data/output.txt', nthread=20) #对input.txt的文件分词输出到output.txt中，使用默认模型和词典，开20个进程"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "啊，这个有点慢了，input.txt只有一句话\n",
    "\n",
    "> 真的处理过分词任务的人都知道 用结巴不是因为它好 而且因为它快... 机器学习的分词算法一直都比结巴好 可惜响应速度跟不上  --网友\n",
    "\n",
    "另外怀疑时间不是按句子个数线性增加的，然后我又向input.txt添了几句话。差不多20句话也是两分钟"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 代码示例5 训练模型\n",
    "这部分因为还没确定msr_training.utf8里面数据的格式，没有训练集，先不做测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pkuseg\n",
    "#训练文件为'msr_training.utf8'，测试文件为'msr_test_gold.utf8'，模型存到'./models'目录下，开20个进程训练模型\n",
    "pkuseg.train('msr_training.utf8', 'msr_test_gold.utf8', './models', nthread=20)\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 参数说明\n",
    "```\n",
    "\tpkuseg.pkuseg(model_name='msra', user_dict='safe_lexicon')\n",
    "\tmodel_name\t\t模型路径。默认是'msra'表示我们预训练好的模型(仅对pip下载的用户)。用户可以填自己下载或训练的模型所在的路径如model_name='./models'。\n",
    "\tuser_dict\t\t设置用户词典。默认为'safe_lexicon'表示我们提供的一个中文词典(仅pip)。用户可以传入一个包含若干自定义单词的迭代器。\n",
    "\t```\n",
    "\t```\n",
    "\tpkuseg.test(readFile, outputFile, model_name='msra', user_dict='safe_lexicon', nthread=10)\n",
    "\treadFile\t\t输入文件路径\n",
    "\toutputFile\t\t输出文件路径\n",
    "\tmodel_name\t\t同pkuseg.pkuseg\n",
    "\tuser_dict\t\t同pkuseg.pkuseg\n",
    "\tnthread\t\t\t测试时开的进程数\n",
    "\t```\n",
    "\t```\n",
    "\tpkuseg.train(trainFile, testFile, savedir, nthread=10)\n",
    "\ttrainFile\t\t训练文件路径\n",
    "\ttestFile\t\t测试文件路径\n",
    "\tsavedir\t\t\t训练模型的保存路径\n",
    "\tnthread\t\t\t训练时开的进程数\n",
    "\t```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 相关论文\n",
    "本工具包基于以下文献：\n",
    "* Xu Sun, Houfeng Wang, Wenjie Li. Fast Online Training with Frequency-Adaptive Learning Rates for Chinese Word Segmentation and New Word Detection. ACL. 253–262. 2012 \n",
    "\n",
    "* Jingjing Xu, Xu Sun. Dependency-based Gated Recursive Neural Network for Chinese Word Segmentation. ACL 2016: 567-572\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 客观看待\n",
    "- 第一条 与其余分词工具包的性能对比是否公平\n",
    "\n",
    " 针对这个问题，有人也在issue提出了质疑，有兴趣大家可以看下，这里不做过多评价\n",
    "- 第二条 不支持词性标注"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
