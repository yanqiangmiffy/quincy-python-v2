{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "原文在这里：\n",
    "\n",
    "https://www.dataapplab.com/introduction-time-series-classification/\n",
    "\n",
    "关于写这篇文章的目的，是因为最近包括之前遇到了中分组时间序列数据，目标是：对每个对象进行分类 数据：每个对象都有多条数据，是时间序列的形式。\n",
    "\n",
    "总结在下深度学习的做法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 简介\n",
    "\n",
    "我们大多数人所接触的时间序列数据主要用于生成预测。无论是预测产品的需求或销售，航空公司的乘客数量，还是某只股票的收盘价，我们都习惯于利用成熟的时间序列技术来预测需求。\n",
    "\n",
    "但是，随着生成的数据量呈指数级增长，尝试新思想和新算法的机会也随之增加。处理复杂的时间序列数据集仍然是一个有潜力的领域，扩展您的库以包含新想法总是有帮助的。\n",
    "\n",
    "我在这篇文章中要做的就是向你们介绍时间序列分类的创新概念。我们将首先了解这个主题的含义和它在行业中的应用。但是，我们不会停留在理论部分—我们将着手处理时间序列数据集并执行二进制时间序列分类。边做边学——这也会帮助你以实践的方式理解这个概念。\n",
    "\n",
    "如果你以前没有研究过时间序列问题，我强烈建议你先从一些基本的预测开始。你可以先看看这篇文章: A comprehensive beginner’s guide to create a Time Series Forecast (with Codes in Python)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目录\n",
    "\n",
    "1. 时间序列分类简介\n",
    "    a. ECG信号\n",
    "    b. 图像数据\n",
    "    c. 传感器\n",
    "2. 问题描述\n",
    "3. 读取和理解数据\n",
    "4. 预先处理\n",
    "5. 建立时间序列分类模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 时间序列分类简介\n",
    "\n",
    " 时间序列分类实际上已经存在一段时间了。但到目前为止，它主要局限于实验室研究，而不是工业应用。不过由于有很多研究正在进行，新的数据集正在创建，一些新的算法也正在提出。当我第一次遇到时间序列分类的概念时，我最初的想法是：我们如何对时间序列进行分类？时间序列分类的数据是什么样子的？\n",
    "\n",
    "可以想象，时间序列分类数据不同于常规分类问题，因为属性具有有序的序列。让我们来看看一些时间序列分类用例来理解这种差异。\n",
    "\n",
    "1) ECG/EEG信号分类\n",
    "\n",
    "心电图（ECG，Electrocardiogram）记录着心脏的电活动，被广泛地用于诊断各种心脏问题。这些心电信号是用外部电极捕捉的。例如，考虑下面的信号样本，它表示一个心跳的电活动。左边的图像表示正常的心跳，而相邻的图像表示心肌梗死。\n",
    "![](https://www.dataapplab.com/wp-content/uploads/2019/07/%E5%BE%AE%E4%BF%A1%E5%9B%BE%E7%89%87_20190722163302.jpg)\n",
    "\n",
    "2) 图像数据\n",
    "\n",
    "图像也可以是时间序列相关的格式。考虑以下场景:\n",
    "\n",
    "作物生长在特定的领域取决于天气条件、土壤肥力、水的可用性和其他外部因素。这片土地的照片是连续5年每天拍摄的，并标有种植在这片土地上的作物的名称。数据集中的图像是在固定的时间间隔后拍摄的，并且有一个确定的序列，这是对图像进行分类的一个重要因素。\n",
    "\n",
    "3) 动作传感器数据分类\n",
    "\n",
    "传感器产生高频数据，可以识别出物体在其范围内的运动。通过设置多个无线传感器，观察传感器信号强度的变化，可以识别出物体的运动方向。\n",
    "\n",
    "你还能想到什么其他的应用我们可以应用时间序列分类？请在文章下面的评论部分告诉我们。\n",
    "\n",
    "问题描述\n",
    "\n",
    "下面我们以“室内用户运动预测”这个问题为例。在这个挑战中，多个运动传感器被放置在不同的房间中，目标是根据从这些运动传感器捕捉到的频率数据来识别一个人是否在房间中移动过。\n",
    "\n",
    "一共有四个运动传感器(A1、A2、A3、A4)分布在两个房间。请看下图，它说明了传感器在每个房间的位置。这两个房间的设置是在3对不同的房间组中创建的(group1、group2、group3)。\n",
    "\n",
    "一个人可以沿着上图所示的六个预定义路径中的任意一条移动。如果一个人走在2号、3号、4号或6号路上，他就会在房间里移动。另一方面，如果一个人遵循路径1或路径5，我们可以说这个人已经在房间之间移动了。\n",
    "\n",
    "传感器读数可以用来识别一个人在给定时间点的位置。当人在房间里走动或穿越房间时，传感器中的读数会发生变化。此更改可用于标识人员的路径。\n",
    "\n",
    "现在问题陈述已经很清楚了，是时候开始编写代码了!在下一节中，我们将查看问题的数据集，它将帮助您清除关于此语句的任何遗留问题。您可以从这个链接下载数据集:https://archive.ics.uci.edu/ml/datasets/Indoor+User+Movement+Prediction+from+RSS+data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 读取和理解数据\n",
    "\n",
    "我们的数据集包括316个文件:\n",
    "\n",
    "     · 314个MovementAAL 数据集，包含环境中运动传感器的读数\n",
    "    · 包含每个MovementAAL文件的目标变量的Target 数据集\n",
    "    · 一个Group数据集来标识不同的移动文件属于哪个设置组\n",
    "    · 包含对象所取路径的路径数据文件\n",
    "\n",
    "让我们看一下数据集。我们将从导入对应的数据库开始。\n",
    "\n",
    "Python Code："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from os import listdir\n",
    "# import tensorflow.keras as keras\n",
    "# from tensorflow.keras.preprocessing import sequence\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras.models import Sequential,Model\n",
    "# from tensorflow.keras.layers import *\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# from tensorflow.keras.models import load_model\n",
    "\n",
    "# from tensorflow.keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from os import listdir\n",
    "import keras\n",
    "from keras.preprocessing import sequence\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential,Model\n",
    "from keras.layers import *\n",
    "from keras.optimizers import Adam\n",
    "from keras.models import load_model\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在载入数据之前，我们一个快速看一下我们要处理的数据。读取移动数据的前两个文件："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('data/MovementAAL/dataset/MovementAAL_RSS_1.csv')\n",
    "df2=pd.read_csv('data/MovementAAL/dataset/MovementAAL_RSS_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.90476</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.28571</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.32</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>0.30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.38095</td>\n",
       "      <td>-0.28</td>\n",
       "      <td>-0.14286</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.28571</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>-0.47619</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.14286</td>\n",
       "      <td>-0.20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.90476         -0.48       0.28571          0.30\n",
       "1      -0.57143         -0.32       0.14286          0.30\n",
       "2      -0.38095         -0.28      -0.14286          0.35\n",
       "3      -0.28571         -0.20      -0.47619          0.35\n",
       "4      -0.14286         -0.20       0.14286         -0.20"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#RSS_anchor1</th>\n",
       "      <th>RSS_anchor2</th>\n",
       "      <th>RSS_anchor3</th>\n",
       "      <th>RSS_anchor4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.57143</td>\n",
       "      <td>-0.20</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.48</td>\n",
       "      <td>0.76190</td>\n",
       "      <td>-0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.85714</td>\n",
       "      <td>-0.60</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.40</td>\n",
       "      <td>0.71429</td>\n",
       "      <td>0.60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.76190</td>\n",
       "      <td>-0.84</td>\n",
       "      <td>0.85714</td>\n",
       "      <td>0.45</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   #RSS_anchor1   RSS_anchor2   RSS_anchor3   RSS_anchor4\n",
       "0      -0.57143         -0.20       0.71429          0.50\n",
       "1      -0.76190         -0.48       0.76190         -0.25\n",
       "2      -0.85714         -0.60       0.85714          0.55\n",
       "3      -0.76190         -0.40       0.71429          0.60\n",
       "4      -0.76190         -0.84       0.85714          0.45"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这些文件包含来自四个传感器(A1、A2、A3、A4)的标准化数据。csv文件的长度(行数)不同，因为每个csv对应的数据的持续时间不同。为了简化问题，让我们假设每秒钟收集一次传感器数据。第一次读取持续时间为27秒(即27行)，而另一次读取持续时间为26秒(即26行)。\n",
    "\n",
    "在建立模型之前，我们必须处理这个变化的长度。现在，我们将使用以下代码块从传感器中读取并存储列表的值:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MovementAAL/dataset/MovementAAL_RSS_1.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_2.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_3.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_4.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_5.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_6.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_7.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_8.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_9.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_10.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_11.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_12.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_13.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_14.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_15.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_16.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_17.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_18.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_19.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_20.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_21.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_22.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_23.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_24.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_25.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_26.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_27.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_28.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_29.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_30.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_31.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_32.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_33.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_34.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_35.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_36.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_37.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_38.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_39.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_40.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_41.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_42.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_43.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_44.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_45.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_46.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_47.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_48.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_49.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_50.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_51.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_52.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_53.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_54.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_55.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_56.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_57.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_58.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_59.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_60.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_61.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_62.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_63.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_64.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_65.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_66.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_67.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_68.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_69.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_70.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_71.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_72.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_73.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_74.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_75.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_76.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_77.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_78.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_79.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_80.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_81.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_82.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_83.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_84.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_85.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_86.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_87.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_88.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_89.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_90.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_91.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_92.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_93.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_94.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_95.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_96.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_97.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_98.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_99.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_100.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_101.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_102.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_103.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_104.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_105.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_106.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_107.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_108.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_109.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_110.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_111.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_112.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_113.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_114.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_115.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_116.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_117.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_118.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_119.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_120.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_121.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_122.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_123.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_124.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_125.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_126.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_127.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_128.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_129.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_130.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_131.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_132.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_133.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_134.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_135.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_136.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_137.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_138.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_139.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_140.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_141.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_142.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_143.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_144.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_145.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_146.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_147.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_148.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_149.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_150.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_151.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_152.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_153.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_154.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_155.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_156.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_157.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_158.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_159.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_160.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_161.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_162.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_163.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_164.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_165.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_166.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_167.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_168.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_169.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_170.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_171.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_172.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_173.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_174.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_175.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_176.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_177.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_178.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_179.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_180.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_181.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_182.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_183.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_184.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_185.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_186.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_187.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_188.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_189.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_190.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_191.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_192.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_193.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_194.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_195.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_196.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_197.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_198.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_199.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_200.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_201.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_202.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_203.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_204.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_205.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_206.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_207.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_208.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_209.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_210.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_211.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_212.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_213.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_214.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_215.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_216.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_217.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_218.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_219.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_220.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_221.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_222.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_223.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_224.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_225.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_226.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_227.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_228.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_229.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_230.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_231.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_232.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_233.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_234.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_235.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_236.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_237.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_238.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_239.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_240.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_241.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_242.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_243.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_244.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_245.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_246.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_247.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_248.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_249.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_250.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_251.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_252.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_253.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_254.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_255.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_256.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_257.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_258.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_259.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_260.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_261.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_262.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_263.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_264.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_265.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_266.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_267.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_268.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_269.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_270.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_271.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_272.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_273.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_274.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_275.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_276.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_277.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_278.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_279.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_280.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_281.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_282.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_283.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_284.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_285.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_286.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_287.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_288.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_289.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_290.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_291.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_292.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_293.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_294.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_295.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_296.csv\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/MovementAAL/dataset/MovementAAL_RSS_297.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_298.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_299.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_300.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_301.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_302.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_303.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_304.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_305.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_306.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_307.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_308.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_309.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_310.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_311.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_312.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_313.csv\n",
      "data/MovementAAL/dataset/MovementAAL_RSS_314.csv\n"
     ]
    }
   ],
   "source": [
    "path='data/MovementAAL/dataset/MovementAAL_RSS_'\n",
    "sequences=list()\n",
    "for i in range(1,315):\n",
    "    file_path=path+str(i)+'.csv'\n",
    "    print(file_path)\n",
    "    df=pd.read_csv(file_path,header=0)\n",
    "    values=df.values\n",
    "    sequences.append(values)\n",
    "targets=pd.read_csv('data/MovementAAL/dataset/MovementAAL_target.csv')\n",
    "targets=targets.values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.90476 , -0.48    ,  0.28571 ,  0.3     ],\n",
       "       [-0.57143 , -0.32    ,  0.14286 ,  0.3     ],\n",
       "       [-0.38095 , -0.28    , -0.14286 ,  0.35    ],\n",
       "       [-0.28571 , -0.2     , -0.47619 ,  0.35    ],\n",
       "       [-0.14286 , -0.2     ,  0.14286 , -0.2     ],\n",
       "       [-0.14286 , -0.2     ,  0.047619,  0.      ],\n",
       "       [-0.14286 , -0.16    , -0.38095 ,  0.2     ],\n",
       "       [-0.14286 , -0.04    , -0.61905 , -0.2     ],\n",
       "       [-0.095238, -0.08    ,  0.14286 , -0.55    ],\n",
       "       [-0.047619,  0.04    , -0.095238,  0.05    ],\n",
       "       [-0.19048 , -0.04    ,  0.095238,  0.4     ],\n",
       "       [-0.095238, -0.04    , -0.14286 ,  0.35    ],\n",
       "       [-0.33333 , -0.08    , -0.28571 , -0.2     ],\n",
       "       [-0.2381  ,  0.04    ,  0.14286 ,  0.35    ],\n",
       "       [ 0.      ,  0.08    ,  0.14286 ,  0.05    ],\n",
       "       [-0.095238,  0.04    ,  0.095238,  0.1     ],\n",
       "       [-0.14286 , -0.2     ,  0.14286 ,  0.5     ],\n",
       "       [-0.19048 ,  0.04    , -0.42857 ,  0.3     ],\n",
       "       [-0.14286 , -0.08    , -0.2381  ,  0.15    ],\n",
       "       [-0.33333 ,  0.16    , -0.14286 , -0.8     ],\n",
       "       [-0.42857 ,  0.16    , -0.28571 , -0.1     ],\n",
       "       [-0.71429 ,  0.16    , -0.28571 ,  0.2     ],\n",
       "       [-0.095238, -0.08    ,  0.095238,  0.35    ],\n",
       "       [-0.28571 ,  0.04    ,  0.14286 ,  0.2     ],\n",
       "       [ 0.      ,  0.04    ,  0.14286 ,  0.1     ],\n",
       "       [ 0.      ,  0.04    , -0.047619, -0.05    ],\n",
       "       [-0.14286 , -0.6     , -0.28571 , -0.1     ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1,\n",
       "       -1, -1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1, -1, -1, -1, -1, -1,\n",
       "       -1, -1, -1, -1, -1, -1, -1, -1], dtype=int64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如前所述，数据集是在三对不同的房间组中收集的，因此分为三组。此信息可用于将数据集划分为训练集、测试集和验证集。我们现在加载数据组csv文件:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups=pd.read_csv('data/MovementAAL/groups/MovementAAL_DatasetGroup.csv',header=0)\n",
    "groups=groups.values[:,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们将取前两组数据用于训练集，取第三组数据用于测试集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 预先处理\n",
    "\n",
    "由于时间序列数据的长度是变化的，我们不能直接在这个数据集上建立模型。那么如何确定一个级数的理想长度呢？我们可以通过多种方式来解决这个问题，这里有一些想法(我很乐意在评论部分听到你的建议):\n",
    "\n",
    "- 用零填充较短的序列，使所有序列的长度相等。但在这种情况下，我们将向模型提供一些不正确的数据\n",
    "- 找出序列的最大长度，并用最后一行中的数据填充序列\n",
    "- 在数据集中确定序列的最小长度，并将所有其他序列截断到该长度。然而，这将导致数据的巨大损失\n",
    "- 取所有长度的平均值，截断较长的序列，填充比平均长度短的序列求出最小、最大和平均长度\n",
    "\n",
    "我们来找出最小、最大和平均长度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    314.000000\n",
       "mean      42.028662\n",
       "std       16.185303\n",
       "min       19.000000\n",
       "25%       26.000000\n",
       "50%       41.000000\n",
       "75%       56.000000\n",
       "max      129.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_sequences=[]\n",
    "for one_seq in sequences:\n",
    "    len_sequences.append(len(one_seq))\n",
    "pd.Series(len_sequences).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "大多数文件的长度在40到60之间。只有3个文件的长度超过100。因此，取最小或最大长度没有多大意义。第90个四分位数是60，我们把这个作为数据的序列长度。让我们把它码出来:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 129, 4)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# padding the sequence with the values in last row to max length\n",
    "to_pad=129\n",
    "new_seq=[]\n",
    "for one_seq in sequences:\n",
    "    len_one_seq=len(one_seq)\n",
    "    last_val=one_seq[-1]\n",
    "    n=to_pad-len_one_seq\n",
    "    to_concat=np.repeat(last_val,n).reshape(4,n).transpose()\n",
    "    new_one_seq=np.concatenate([one_seq,to_concat])\n",
    "    new_seq.append(new_one_seq)\n",
    "\n",
    "final_seq=np.stack(new_seq)\n",
    "# final_seq.shape (314, 129, 4)\n",
    "final_seq.shape\n",
    "# 进行截断\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 60, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_len=60\n",
    "final_seq=sequence.pad_sequences(final_seq,maxlen=seq_len,padding='post',\n",
    "                                dtype='float',truncating='post')\n",
    "final_seq.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们的数据集已经准备好，我们将会根据Group将它分开。准备好训练集，验证集和测试集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train.shape: (106, 60, 4)\n",
      "validation.shape: (104, 60, 4)\n",
      "test.num: (104, 60, 4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
       "       1., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train=[final_seq[i] for i in range(len(groups)) if(groups[i])==2]\n",
    "validation=[final_seq[i] for i in range(len(groups)) if(groups[i])==1]\n",
    "test=[final_seq[i] for i in range(len(groups)) if(groups[i])==3]\n",
    "\n",
    "\n",
    "\n",
    "train_target=[targets[i] for i in range(len(groups)) if(groups[i])==2]\n",
    "validation_target=[targets[i] for i in range(len(groups)) if(groups[i])==1]\n",
    "test_target=[targets[i] for i in range(len(groups)) if(groups[i])==3]\n",
    "\n",
    "train=np.array(train)\n",
    "validation=np.array(validation)\n",
    "test=np.array(test)\n",
    "\n",
    "print(\"train.shape:\",train.shape)\n",
    "print(\"validation.shape:\",validation.shape)\n",
    "print(\"test.num:\",test.shape)\n",
    "\n",
    "train_target=np.array(train_target)\n",
    "validation_target=np.array(validation_target)\n",
    "test_target=np.array(test_target)\n",
    "\n",
    "train_target=(train_target+1)/2\n",
    "validation_target=(validation_target+1)/2\n",
    "test_target=(test_target+1)/2\n",
    "test_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立时间序列分类模型\n",
    "\n",
    "我们已经准备好了用于LSTM(长短期内存)模型的数据。我们处理可变长度序列并创建了训练、验证和测试集。让我们构建一个单层LSTM网络。\n",
    "\n",
    "注意:您可以在本教程中熟悉LSTMs。我建议您首先完成这些操作，因为这将帮助您理解下面的代码是如何工作的。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model=Sequential()\n",
    "# model.add(LSTM(256,input_shape=(seq_len,4)))\n",
    "# model.add(Dense(1,activation='sigmoid'))\n",
    "# model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_layer (InputLayer)     [(None, 60, 4)]           0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  (None, 256)               267264    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 267,521\n",
      "Trainable params: 267,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input_layer=Input(shape=(seq_len,4),name=\"input_layer\")\n",
    "lstm_layer=LSTM(256)(input_layer)\n",
    "pred=Dense(1,activation='sigmoid')(lstm_layer)\n",
    "model=Model(input_layer,pred)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam=Adam(lr=0.001)\n",
    "checkpoint=ModelCheckpoint('data/best_model.pkl',monitor='accuracy',save_best_only=True,verbose=1)\n",
    "model.compile(loss=\"binary_crossentropy\",optimizer=adam,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6999 - accuracy: 0.4375\n",
      "Epoch 00001: accuracy improved from -inf to 0.48113, saving model to data/best_model.pkl\n",
      "WARNING:tensorflow:From E:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py:1817: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 764ms/step - loss: 0.7022 - accuracy: 0.4811 - val_loss: 0.6712 - val_accuracy: 0.5962\n",
      "Epoch 2/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6753 - accuracy: 0.6562\n",
      "Epoch 00002: accuracy improved from 0.48113 to 0.68868, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 684ms/step - loss: 0.6554 - accuracy: 0.6887 - val_loss: 0.6552 - val_accuracy: 0.5385\n",
      "Epoch 3/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6288 - accuracy: 0.6250\n",
      "Epoch 00003: accuracy did not improve from 0.68868\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5638 - accuracy: 0.6604 - val_loss: 1.7602 - val_accuracy: 0.5385\n",
      "Epoch 4/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.7664 - accuracy: 0.7188\n",
      "Epoch 00004: accuracy improved from 0.68868 to 0.73585, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 647ms/step - loss: 0.5876 - accuracy: 0.7358 - val_loss: 0.6970 - val_accuracy: 0.5865\n",
      "Epoch 5/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5093 - accuracy: 0.8438\n",
      "Epoch 00005: accuracy improved from 0.73585 to 0.78302, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 700ms/step - loss: 0.5778 - accuracy: 0.7830 - val_loss: 0.7277 - val_accuracy: 0.5577\n",
      "Epoch 6/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5599 - accuracy: 0.8125\n",
      "Epoch 00006: accuracy did not improve from 0.78302\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5849 - accuracy: 0.7736 - val_loss: 0.6846 - val_accuracy: 0.5577\n",
      "Epoch 7/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4620 - accuracy: 0.9062\n",
      "Epoch 00007: accuracy improved from 0.78302 to 0.83019, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.5307 - accuracy: 0.8302 - val_loss: 0.7806 - val_accuracy: 0.5865\n",
      "Epoch 8/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3967 - accuracy: 0.8438\n",
      "Epoch 00008: accuracy did not improve from 0.83019\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4274 - accuracy: 0.8113 - val_loss: 1.1847 - val_accuracy: 0.5865\n",
      "Epoch 9/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4771 - accuracy: 0.7500\n",
      "Epoch 00009: accuracy did not improve from 0.83019\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4712 - accuracy: 0.7830 - val_loss: 0.8495 - val_accuracy: 0.5577\n",
      "Epoch 10/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3058 - accuracy: 0.9375\n",
      "Epoch 00010: accuracy did not improve from 0.83019\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4162 - accuracy: 0.8208 - val_loss: 0.8692 - val_accuracy: 0.5962\n",
      "Epoch 11/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4571 - accuracy: 0.7812\n",
      "Epoch 00011: accuracy improved from 0.83019 to 0.83962, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 670ms/step - loss: 0.3941 - accuracy: 0.8396 - val_loss: 0.9196 - val_accuracy: 0.5673\n",
      "Epoch 12/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3390 - accuracy: 0.8750\n",
      "Epoch 00012: accuracy improved from 0.83962 to 0.84906, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 678ms/step - loss: 0.3611 - accuracy: 0.8491 - val_loss: 1.0926 - val_accuracy: 0.5577\n",
      "Epoch 13/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3536 - accuracy: 0.8438\n",
      "Epoch 00013: accuracy did not improve from 0.84906\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3557 - accuracy: 0.8113 - val_loss: 0.9537 - val_accuracy: 0.5865\n",
      "Epoch 14/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2398 - accuracy: 0.8750\n",
      "Epoch 00014: accuracy did not improve from 0.84906\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3857 - accuracy: 0.8396 - val_loss: 0.7277 - val_accuracy: 0.5865\n",
      "Epoch 15/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3219 - accuracy: 0.9062\n",
      "Epoch 00015: accuracy did not improve from 0.84906\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3852 - accuracy: 0.8491 - val_loss: 0.7240 - val_accuracy: 0.5865\n",
      "Epoch 16/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4062 - accuracy: 0.8438\n",
      "Epoch 00016: accuracy improved from 0.84906 to 0.86792, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 671ms/step - loss: 0.3601 - accuracy: 0.8679 - val_loss: 0.8019 - val_accuracy: 0.5577\n",
      "Epoch 17/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3339 - accuracy: 0.8750\n",
      "Epoch 00017: accuracy did not improve from 0.86792\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3657 - accuracy: 0.8679 - val_loss: 1.1444 - val_accuracy: 0.5962\n",
      "Epoch 18/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2492 - accuracy: 0.9375\n",
      "Epoch 00018: accuracy did not improve from 0.86792\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4352 - accuracy: 0.8491 - val_loss: 1.4641 - val_accuracy: 0.5288\n",
      "Epoch 19/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4776 - accuracy: 0.8438\n",
      "Epoch 00019: accuracy did not improve from 0.86792\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3811 - accuracy: 0.8491 - val_loss: 0.7677 - val_accuracy: 0.6250\n",
      "Epoch 20/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3216 - accuracy: 0.8750\n",
      "Epoch 00020: accuracy improved from 0.86792 to 0.87736, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 686ms/step - loss: 0.3245 - accuracy: 0.8774 - val_loss: 0.8182 - val_accuracy: 0.6346\n",
      "Epoch 21/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2208 - accuracy: 0.9688\n",
      "Epoch 00021: accuracy improved from 0.87736 to 0.91509, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 672ms/step - loss: 0.2767 - accuracy: 0.9151 - val_loss: 1.2453 - val_accuracy: 0.5577\n",
      "Epoch 22/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1936 - accuracy: 0.9062\n",
      "Epoch 00022: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3853 - accuracy: 0.8491 - val_loss: 0.9896 - val_accuracy: 0.6058\n",
      "Epoch 23/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4202 - accuracy: 0.8438\n",
      "Epoch 00023: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3462 - accuracy: 0.8774 - val_loss: 0.8381 - val_accuracy: 0.6538\n",
      "Epoch 24/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4360 - accuracy: 0.7500\n",
      "Epoch 00024: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4604 - accuracy: 0.7642 - val_loss: 0.7059 - val_accuracy: 0.5865\n",
      "Epoch 25/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3893 - accuracy: 0.9062\n",
      "Epoch 00025: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4142 - accuracy: 0.8396 - val_loss: 0.7625 - val_accuracy: 0.5865\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3866 - accuracy: 0.8438\n",
      "Epoch 00026: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2979 - accuracy: 0.9057 - val_loss: 1.2464 - val_accuracy: 0.5673\n",
      "Epoch 27/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1403 - accuracy: 1.0000\n",
      "Epoch 00027: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2684 - accuracy: 0.9151 - val_loss: 0.7787 - val_accuracy: 0.7115\n",
      "Epoch 28/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4502 - accuracy: 0.8125\n",
      "Epoch 00028: accuracy did not improve from 0.91509\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2700 - accuracy: 0.8868 - val_loss: 1.5923 - val_accuracy: 0.5865\n",
      "Epoch 29/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2304 - accuracy: 0.8750\n",
      "Epoch 00029: accuracy improved from 0.91509 to 0.93396, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 696ms/step - loss: 0.1800 - accuracy: 0.9340 - val_loss: 0.6875 - val_accuracy: 0.7019\n",
      "Epoch 30/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1831 - accuracy: 0.9062\n",
      "Epoch 00030: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3292 - accuracy: 0.8113 - val_loss: 0.6584 - val_accuracy: 0.6827\n",
      "Epoch 31/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2884 - accuracy: 0.8438\n",
      "Epoch 00031: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2776 - accuracy: 0.8396 - val_loss: 0.8589 - val_accuracy: 0.5962\n",
      "Epoch 32/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3484 - accuracy: 0.8438\n",
      "Epoch 00032: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2801 - accuracy: 0.9151 - val_loss: 1.6523 - val_accuracy: 0.5962\n",
      "Epoch 33/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5152 - accuracy: 0.8125\n",
      "Epoch 00033: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4022 - accuracy: 0.8396 - val_loss: 1.6981 - val_accuracy: 0.6058\n",
      "Epoch 34/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2588 - accuracy: 0.9375\n",
      "Epoch 00034: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2746 - accuracy: 0.9057 - val_loss: 1.0127 - val_accuracy: 0.6538\n",
      "Epoch 35/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1998 - accuracy: 0.9375\n",
      "Epoch 00035: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2509 - accuracy: 0.8774 - val_loss: 1.5658 - val_accuracy: 0.5962\n",
      "Epoch 36/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3691 - accuracy: 0.8438\n",
      "Epoch 00036: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2817 - accuracy: 0.9151 - val_loss: 0.7114 - val_accuracy: 0.6827\n",
      "Epoch 37/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2845 - accuracy: 0.8750\n",
      "Epoch 00037: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2398 - accuracy: 0.8868 - val_loss: 0.7547 - val_accuracy: 0.7019\n",
      "Epoch 38/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2055 - accuracy: 0.9062\n",
      "Epoch 00038: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2411 - accuracy: 0.9151 - val_loss: 0.9203 - val_accuracy: 0.6923\n",
      "Epoch 39/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1856 - accuracy: 0.9062\n",
      "Epoch 00039: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2114 - accuracy: 0.9151 - val_loss: 1.0704 - val_accuracy: 0.6442\n",
      "Epoch 40/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2296 - accuracy: 0.8750\n",
      "Epoch 00040: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2246 - accuracy: 0.8491 - val_loss: 1.1358 - val_accuracy: 0.5962\n",
      "Epoch 41/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3042 - accuracy: 0.8125\n",
      "Epoch 00041: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2791 - accuracy: 0.8208 - val_loss: 1.0291 - val_accuracy: 0.6346\n",
      "Epoch 42/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1020 - accuracy: 0.9375\n",
      "Epoch 00042: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1595 - accuracy: 0.8962 - val_loss: 1.3900 - val_accuracy: 0.6346\n",
      "Epoch 43/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2317 - accuracy: 0.9062\n",
      "Epoch 00043: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2262 - accuracy: 0.8962 - val_loss: 1.4556 - val_accuracy: 0.5673\n",
      "Epoch 44/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1586 - accuracy: 0.8750\n",
      "Epoch 00044: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2870 - accuracy: 0.8585 - val_loss: 1.9772 - val_accuracy: 0.5096\n",
      "Epoch 45/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6187 - accuracy: 0.7188\n",
      "Epoch 00045: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5950 - accuracy: 0.7170 - val_loss: 1.2479 - val_accuracy: 0.5288\n",
      "Epoch 46/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3679 - accuracy: 0.8438\n",
      "Epoch 00046: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4344 - accuracy: 0.7736 - val_loss: 0.7398 - val_accuracy: 0.5865\n",
      "Epoch 47/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5043 - accuracy: 0.7188\n",
      "Epoch 00047: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.4484 - accuracy: 0.7830 - val_loss: 0.7490 - val_accuracy: 0.5481\n",
      "Epoch 48/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3446 - accuracy: 0.9062\n",
      "Epoch 00048: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3896 - accuracy: 0.8113 - val_loss: 0.9594 - val_accuracy: 0.5481\n",
      "Epoch 49/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4169 - accuracy: 0.8125\n",
      "Epoch 00049: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3138 - accuracy: 0.8585 - val_loss: 1.2235 - val_accuracy: 0.5288\n",
      "Epoch 50/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2715 - accuracy: 0.8438\n",
      "Epoch 00050: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2891 - accuracy: 0.8774 - val_loss: 1.5853 - val_accuracy: 0.5288\n",
      "Epoch 51/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1849 - accuracy: 0.9375\n",
      "Epoch 00051: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3194 - accuracy: 0.8962 - val_loss: 1.7916 - val_accuracy: 0.6058\n",
      "Epoch 52/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5039 - accuracy: 0.8438\n",
      "Epoch 00052: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4206 - accuracy: 0.8396 - val_loss: 1.2626 - val_accuracy: 0.6154\n",
      "Epoch 53/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4952 - accuracy: 0.7812\n",
      "Epoch 00053: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3706 - accuracy: 0.8491 - val_loss: 1.7012 - val_accuracy: 0.6346\n",
      "Epoch 54/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2961 - accuracy: 0.8750\n",
      "Epoch 00054: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2918 - accuracy: 0.8962 - val_loss: 1.6230 - val_accuracy: 0.6538\n",
      "Epoch 55/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3145 - accuracy: 0.8750\n",
      "Epoch 00055: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2814 - accuracy: 0.8868 - val_loss: 1.5602 - val_accuracy: 0.6538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 56/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2548 - accuracy: 0.9062\n",
      "Epoch 00056: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.2927 - accuracy: 0.8774 - val_loss: 1.6250 - val_accuracy: 0.6346\n",
      "Epoch 57/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3246 - accuracy: 0.8750\n",
      "Epoch 00057: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3202 - accuracy: 0.8679 - val_loss: 1.7557 - val_accuracy: 0.6058\n",
      "Epoch 58/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2367 - accuracy: 0.9688\n",
      "Epoch 00058: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3461 - accuracy: 0.8774 - val_loss: 1.7536 - val_accuracy: 0.6635\n",
      "Epoch 59/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3716 - accuracy: 0.9062\n",
      "Epoch 00059: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3971 - accuracy: 0.8679 - val_loss: 0.4975 - val_accuracy: 0.7692\n",
      "Epoch 60/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4695 - accuracy: 0.7812\n",
      "Epoch 00060: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4710 - accuracy: 0.7830 - val_loss: 0.5080 - val_accuracy: 0.7885\n",
      "Epoch 61/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5274 - accuracy: 0.7188\n",
      "Epoch 00061: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.5883 - accuracy: 0.6792 - val_loss: 0.5464 - val_accuracy: 0.7212\n",
      "Epoch 62/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.5720 - accuracy: 0.6875\n",
      "Epoch 00062: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5419 - accuracy: 0.6981 - val_loss: 0.5521 - val_accuracy: 0.7115\n",
      "Epoch 63/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4646 - accuracy: 0.7500\n",
      "Epoch 00063: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.5150 - accuracy: 0.7170 - val_loss: 0.6787 - val_accuracy: 0.6731\n",
      "Epoch 64/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4628 - accuracy: 0.7188\n",
      "Epoch 00064: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4996 - accuracy: 0.7075 - val_loss: 0.8068 - val_accuracy: 0.6538\n",
      "Epoch 65/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.6207 - accuracy: 0.6875\n",
      "Epoch 00065: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4798 - accuracy: 0.7830 - val_loss: 0.9941 - val_accuracy: 0.6154\n",
      "Epoch 66/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4567 - accuracy: 0.7500\n",
      "Epoch 00066: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4517 - accuracy: 0.8113 - val_loss: 1.1082 - val_accuracy: 0.6442\n",
      "Epoch 67/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3614 - accuracy: 0.8750\n",
      "Epoch 00067: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4242 - accuracy: 0.8208 - val_loss: 0.8288 - val_accuracy: 0.6731\n",
      "Epoch 68/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4474 - accuracy: 0.7812\n",
      "Epoch 00068: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4234 - accuracy: 0.8302 - val_loss: 0.9487 - val_accuracy: 0.6442\n",
      "Epoch 69/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4226 - accuracy: 0.7188\n",
      "Epoch 00069: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4371 - accuracy: 0.8019 - val_loss: 0.8162 - val_accuracy: 0.6923\n",
      "Epoch 70/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3647 - accuracy: 0.8438\n",
      "Epoch 00070: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3836 - accuracy: 0.8208 - val_loss: 0.6791 - val_accuracy: 0.6538\n",
      "Epoch 71/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.4279 - accuracy: 0.7812\n",
      "Epoch 00071: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.4439 - accuracy: 0.7547 - val_loss: 0.8408 - val_accuracy: 0.5865\n",
      "Epoch 72/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3703 - accuracy: 0.7812\n",
      "Epoch 00072: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3602 - accuracy: 0.7642 - val_loss: 0.9817 - val_accuracy: 0.5673\n",
      "Epoch 73/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2596 - accuracy: 0.8438\n",
      "Epoch 00073: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3194 - accuracy: 0.8113 - val_loss: 0.9241 - val_accuracy: 0.5865\n",
      "Epoch 74/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3747 - accuracy: 0.7500\n",
      "Epoch 00074: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3175 - accuracy: 0.8113 - val_loss: 1.1447 - val_accuracy: 0.5673\n",
      "Epoch 75/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2601 - accuracy: 0.8750\n",
      "Epoch 00075: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 14ms/step - loss: 0.3497 - accuracy: 0.8113 - val_loss: 1.1890 - val_accuracy: 0.5962\n",
      "Epoch 76/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2501 - accuracy: 0.9062\n",
      "Epoch 00076: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2751 - accuracy: 0.8962 - val_loss: 0.7938 - val_accuracy: 0.7404\n",
      "Epoch 77/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1631 - accuracy: 0.9375\n",
      "Epoch 00077: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3667 - accuracy: 0.8302 - val_loss: 0.8429 - val_accuracy: 0.6827\n",
      "Epoch 78/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2716 - accuracy: 0.9062\n",
      "Epoch 00078: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3067 - accuracy: 0.8679 - val_loss: 0.9838 - val_accuracy: 0.6346\n",
      "Epoch 79/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2393 - accuracy: 0.9062\n",
      "Epoch 00079: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2484 - accuracy: 0.9151 - val_loss: 1.4885 - val_accuracy: 0.5385\n",
      "Epoch 80/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3555 - accuracy: 0.9375\n",
      "Epoch 00080: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3280 - accuracy: 0.9151 - val_loss: 1.1873 - val_accuracy: 0.5769\n",
      "Epoch 81/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2793 - accuracy: 0.8750\n",
      "Epoch 00081: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2922 - accuracy: 0.9245 - val_loss: 1.0819 - val_accuracy: 0.5865\n",
      "Epoch 82/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2836 - accuracy: 0.9375\n",
      "Epoch 00082: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2880 - accuracy: 0.9057 - val_loss: 0.7946 - val_accuracy: 0.6058\n",
      "Epoch 83/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3252 - accuracy: 0.9062\n",
      "Epoch 00083: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3294 - accuracy: 0.8774 - val_loss: 0.7198 - val_accuracy: 0.6250\n",
      "Epoch 84/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3183 - accuracy: 0.8438\n",
      "Epoch 00084: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3166 - accuracy: 0.8774 - val_loss: 0.7619 - val_accuracy: 0.6058\n",
      "Epoch 85/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3091 - accuracy: 0.9062\n",
      "Epoch 00085: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2505 - accuracy: 0.9151 - val_loss: 0.9580 - val_accuracy: 0.5769\n",
      "Epoch 86/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2288 - accuracy: 0.9375\n",
      "Epoch 00086: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2331 - accuracy: 0.8962 - val_loss: 0.9915 - val_accuracy: 0.6058\n",
      "Epoch 87/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2556 - accuracy: 0.8750\n",
      "Epoch 00087: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2061 - accuracy: 0.8962 - val_loss: 1.0828 - val_accuracy: 0.6346\n",
      "Epoch 88/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1987 - accuracy: 0.8750\n",
      "Epoch 00088: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1785 - accuracy: 0.8868 - val_loss: 1.3654 - val_accuracy: 0.6442\n",
      "Epoch 89/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3463 - accuracy: 0.8438\n",
      "Epoch 00089: accuracy did not improve from 0.93396\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2052 - accuracy: 0.9340 - val_loss: 1.1865 - val_accuracy: 0.6154\n",
      "Epoch 90/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1278 - accuracy: 1.0000\n",
      "Epoch 00090: accuracy improved from 0.93396 to 0.95283, saving model to data/best_model.pkl\n",
      "INFO:tensorflow:Assets written to: data/best_model.pkl\\assets\n",
      "4/4 [==============================] - 3s 666ms/step - loss: 0.1407 - accuracy: 0.9528 - val_loss: 1.1415 - val_accuracy: 0.6442\n",
      "Epoch 91/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1531 - accuracy: 0.9688\n",
      "Epoch 00091: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.1798 - accuracy: 0.9434 - val_loss: 0.8426 - val_accuracy: 0.6442\n",
      "Epoch 92/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3326 - accuracy: 0.8438\n",
      "Epoch 00092: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.3627 - accuracy: 0.7830 - val_loss: 0.9280 - val_accuracy: 0.6058\n",
      "Epoch 93/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3795 - accuracy: 0.7812\n",
      "Epoch 00093: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.3360 - accuracy: 0.8019 - val_loss: 1.5873 - val_accuracy: 0.5769\n",
      "Epoch 94/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3988 - accuracy: 0.8438\n",
      "Epoch 00094: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.4338 - accuracy: 0.8302 - val_loss: 1.5452 - val_accuracy: 0.5096\n",
      "Epoch 95/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2769 - accuracy: 0.8750\n",
      "Epoch 00095: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2966 - accuracy: 0.8868 - val_loss: 0.7630 - val_accuracy: 0.6538\n",
      "Epoch 96/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.3150 - accuracy: 0.9062\n",
      "Epoch 00096: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2480 - accuracy: 0.9245 - val_loss: 0.6614 - val_accuracy: 0.7404\n",
      "Epoch 97/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2042 - accuracy: 0.9062\n",
      "Epoch 00097: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 12ms/step - loss: 0.2131 - accuracy: 0.9151 - val_loss: 0.9070 - val_accuracy: 0.7019\n",
      "Epoch 98/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.1745 - accuracy: 0.9375\n",
      "Epoch 00098: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2116 - accuracy: 0.9151 - val_loss: 0.9149 - val_accuracy: 0.7212\n",
      "Epoch 99/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2343 - accuracy: 0.9375\n",
      "Epoch 00099: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.1849 - accuracy: 0.9434 - val_loss: 0.9408 - val_accuracy: 0.7404\n",
      "Epoch 100/100\n",
      "1/4 [======>.......................] - ETA: 0s - loss: 0.2671 - accuracy: 0.8750\n",
      "Epoch 00100: accuracy did not improve from 0.95283\n",
      "4/4 [==============================] - 0s 13ms/step - loss: 0.2468 - accuracy: 0.9057 - val_loss: 0.9040 - val_accuracy: 0.6731\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x2509161ec48>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    train,\n",
    "    train_target,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[checkpoint],\n",
    "    validation_data=(validation,validation_target)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 加载以前保存的权重\n",
    "model=tf.keras.models.load_model('./data/best_model.pkl')\n",
    "from sklearn.metrics import accuracy_score\n",
    "test_preds=model.predict(test)\n",
    "test_preds = (test_preds >= 0.5).astype(np.int)\n",
    "accuracy_score(test_target,test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
