{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、余弦相似度\n",
    "\n",
    "2、曼哈顿相似度\n",
    "\n",
    "3、n元语法相似度计算\n",
    "\n",
    "4、jaccard相似性\n",
    "\n",
    "5、 masi距离\n",
    "\n",
    "6、编辑距离"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 利用doc2vec计算余弦相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gensim.models import doc2vec\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>review</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5814_8</td>\n",
       "      <td>1</td>\n",
       "      <td>With all this stuff going down at the moment w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2381_9</td>\n",
       "      <td>1</td>\n",
       "      <td>\\The Classic War of the Worlds\\\" by Timothy Hi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7759_3</td>\n",
       "      <td>0</td>\n",
       "      <td>The film starts with a manager (Nicholas Bell)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3630_4</td>\n",
       "      <td>0</td>\n",
       "      <td>It must be assumed that those who praised this...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9495_8</td>\n",
       "      <td>1</td>\n",
       "      <td>Superbly trashy and wondrously unpretentious 8...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  sentiment                                             review\n",
       "0  5814_8          1  With all this stuff going down at the moment w...\n",
       "1  2381_9          1  \\The Classic War of the Worlds\\\" by Timothy Hi...\n",
       "2  7759_3          0  The film starts with a manager (Nicholas Bell)...\n",
       "3  3630_4          0  It must be assumed that those who praised this...\n",
       "4  9495_8          1  Superbly trashy and wondrously unpretentious 8..."
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 读取数据\n",
    "dataset=pd.read_csv('data/dataset.csv',sep='\\t')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-13 23:18:24,558 : INFO : Building Doc2Vec vocabulary\n",
      "2019-06-13 23:18:24,560 : INFO : collecting all words and their counts\n",
      "2019-06-13 23:18:24,563 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2019-06-13 23:18:25,699 : INFO : PROGRESS: at example #10000, processed 2354776 words (2076264/s), 161418 word types, 10000 tags\n",
      "2019-06-13 23:18:27,181 : INFO : PROGRESS: at example #20000, processed 4686255 words (1574970/s), 249262 word types, 20000 tags\n",
      "2019-06-13 23:18:27,756 : INFO : collected 286719 word types and 25000 unique tags from a corpus of 25000 examples and 5844691 words\n",
      "2019-06-13 23:18:27,759 : INFO : Loading a fresh vocabulary\n",
      "2019-06-13 23:18:35,931 : INFO : effective_min_count=1 retains 286719 unique words (100% of original 286719, drops 0)\n",
      "2019-06-13 23:18:35,933 : INFO : effective_min_count=1 leaves 5844691 word corpus (100% of original 5844691, drops 0)\n",
      "2019-06-13 23:18:38,027 : INFO : deleting the raw counts dictionary of 286719 items\n",
      "2019-06-13 23:18:38,036 : INFO : sample=0.001 downsamples 43 most-common words\n",
      "2019-06-13 23:18:38,038 : INFO : downsampling leaves estimated 4628876 word corpus (79.2% of prior 5844691)\n",
      "2019-06-13 23:18:39,782 : INFO : estimated required memory for 286719 words and 100 dimensions: 387734700 bytes\n",
      "2019-06-13 23:18:39,787 : INFO : resetting layer weights\n",
      "2019-06-13 23:18:45,473 : INFO : Training Doc2Vec model\n",
      "2019-06-13 23:18:45,474 : INFO : training model with 5 workers on 286719 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=5\n",
      "2019-06-13 23:18:46,503 : INFO : EPOCH 1 - PROGRESS: at 6.22% examples, 284091 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:47,510 : INFO : EPOCH 1 - PROGRESS: at 13.19% examples, 304950 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:48,541 : INFO : EPOCH 1 - PROGRESS: at 20.80% examples, 321953 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:49,557 : INFO : EPOCH 1 - PROGRESS: at 29.66% examples, 343063 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:50,570 : INFO : EPOCH 1 - PROGRESS: at 37.76% examples, 348778 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:51,590 : INFO : EPOCH 1 - PROGRESS: at 45.54% examples, 349385 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:52,611 : INFO : EPOCH 1 - PROGRESS: at 53.06% examples, 349691 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:53,614 : INFO : EPOCH 1 - PROGRESS: at 60.32% examples, 347884 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:18:54,624 : INFO : EPOCH 1 - PROGRESS: at 68.97% examples, 353103 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:55,644 : INFO : EPOCH 1 - PROGRESS: at 77.45% examples, 356100 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:56,672 : INFO : EPOCH 1 - PROGRESS: at 86.13% examples, 359709 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:57,680 : INFO : EPOCH 1 - PROGRESS: at 95.78% examples, 365961 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:18:58,201 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-13 23:18:58,233 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-13 23:18:58,252 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-13 23:18:58,273 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-13 23:18:58,297 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-13 23:18:58,298 : INFO : EPOCH - 1 : training on 5844691 raw words (4654193 effective words) took 12.8s, 363126 effective words/s\n",
      "2019-06-13 23:18:59,333 : INFO : EPOCH 2 - PROGRESS: at 7.49% examples, 348696 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:00,361 : INFO : EPOCH 2 - PROGRESS: at 13.76% examples, 318755 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:01,365 : INFO : EPOCH 2 - PROGRESS: at 17.83% examples, 277223 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:19:02,421 : INFO : EPOCH 2 - PROGRESS: at 22.11% examples, 255449 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:03,480 : INFO : EPOCH 2 - PROGRESS: at 27.70% examples, 252856 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:04,493 : INFO : EPOCH 2 - PROGRESS: at 33.08% examples, 251965 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:05,498 : INFO : EPOCH 2 - PROGRESS: at 39.81% examples, 260359 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:06,532 : INFO : EPOCH 2 - PROGRESS: at 45.54% examples, 259908 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:07,537 : INFO : EPOCH 2 - PROGRESS: at 51.66% examples, 262845 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:08,552 : INFO : EPOCH 2 - PROGRESS: at 56.69% examples, 260371 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:09,569 : INFO : EPOCH 2 - PROGRESS: at 62.09% examples, 258409 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:10,610 : INFO : EPOCH 2 - PROGRESS: at 67.31% examples, 256252 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:11,845 : INFO : EPOCH 2 - PROGRESS: at 72.69% examples, 251374 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:12,878 : INFO : EPOCH 2 - PROGRESS: at 80.10% examples, 256637 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:13,888 : INFO : EPOCH 2 - PROGRESS: at 88.44% examples, 265550 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:14,978 : INFO : EPOCH 2 - PROGRESS: at 95.11% examples, 266127 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:15,857 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-13 23:19:15,864 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-13 23:19:15,901 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-13 23:19:15,915 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-13 23:19:15,972 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-13 23:19:15,974 : INFO : EPOCH - 2 : training on 5844691 raw words (4653737 effective words) took 17.7s, 263612 effective words/s\n",
      "2019-06-13 23:19:17,052 : INFO : EPOCH 3 - PROGRESS: at 5.74% examples, 253461 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:18,122 : INFO : EPOCH 3 - PROGRESS: at 11.34% examples, 250815 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:19,127 : INFO : EPOCH 3 - PROGRESS: at 17.18% examples, 260043 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:19:20,128 : INFO : EPOCH 3 - PROGRESS: at 24.97% examples, 283934 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:21,131 : INFO : EPOCH 3 - PROGRESS: at 30.54% examples, 280029 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:22,140 : INFO : EPOCH 3 - PROGRESS: at 37.18% examples, 285217 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:23,170 : INFO : EPOCH 3 - PROGRESS: at 43.46% examples, 284406 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:24,200 : INFO : EPOCH 3 - PROGRESS: at 49.44% examples, 283057 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:25,204 : INFO : EPOCH 3 - PROGRESS: at 54.19% examples, 276733 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:26,221 : INFO : EPOCH 3 - PROGRESS: at 61.18% examples, 280624 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:27,225 : INFO : EPOCH 3 - PROGRESS: at 69.93% examples, 291709 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:28,233 : INFO : EPOCH 3 - PROGRESS: at 78.11% examples, 298352 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:29,250 : INFO : EPOCH 3 - PROGRESS: at 86.31% examples, 304348 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:30,273 : INFO : EPOCH 3 - PROGRESS: at 94.79% examples, 309460 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:30,726 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-13 23:19:30,737 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-13 23:19:30,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-13 23:19:30,812 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-13 23:19:30,816 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-13 23:19:30,818 : INFO : EPOCH - 3 : training on 5844691 raw words (4654082 effective words) took 14.8s, 314041 effective words/s\n",
      "2019-06-13 23:19:31,878 : INFO : EPOCH 4 - PROGRESS: at 5.54% examples, 246507 words/s, in_qsize 9, out_qsize 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-06-13 23:19:32,881 : INFO : EPOCH 4 - PROGRESS: at 11.20% examples, 255166 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:33,893 : INFO : EPOCH 4 - PROGRESS: at 16.56% examples, 255039 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:34,991 : INFO : EPOCH 4 - PROGRESS: at 22.46% examples, 255245 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:36,011 : INFO : EPOCH 4 - PROGRESS: at 30.54% examples, 277095 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:37,045 : INFO : EPOCH 4 - PROGRESS: at 37.69% examples, 285285 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:19:38,096 : INFO : EPOCH 4 - PROGRESS: at 45.51% examples, 293569 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:19:39,112 : INFO : EPOCH 4 - PROGRESS: at 51.46% examples, 291396 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:40,116 : INFO : EPOCH 4 - PROGRESS: at 60.66% examples, 306189 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:41,124 : INFO : EPOCH 4 - PROGRESS: at 67.49% examples, 306626 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:42,133 : INFO : EPOCH 4 - PROGRESS: at 76.95% examples, 318037 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:43,144 : INFO : EPOCH 4 - PROGRESS: at 86.46% examples, 327995 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:44,163 : INFO : EPOCH 4 - PROGRESS: at 95.78% examples, 334674 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:44,526 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-13 23:19:44,543 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-13 23:19:44,568 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-13 23:19:44,573 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-13 23:19:44,575 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-13 23:19:44,576 : INFO : EPOCH - 4 : training on 5844691 raw words (4653825 effective words) took 13.8s, 338426 effective words/s\n",
      "2019-06-13 23:19:45,627 : INFO : EPOCH 5 - PROGRESS: at 9.02% examples, 410039 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:46,631 : INFO : EPOCH 5 - PROGRESS: at 18.66% examples, 433677 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:47,644 : INFO : EPOCH 5 - PROGRESS: at 27.54% examples, 425004 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:48,649 : INFO : EPOCH 5 - PROGRESS: at 37.00% examples, 429965 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:49,657 : INFO : EPOCH 5 - PROGRESS: at 46.40% examples, 429247 words/s, in_qsize 10, out_qsize 0\n",
      "2019-06-13 23:19:50,674 : INFO : EPOCH 5 - PROGRESS: at 55.56% examples, 429167 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:51,678 : INFO : EPOCH 5 - PROGRESS: at 65.00% examples, 429109 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:52,687 : INFO : EPOCH 5 - PROGRESS: at 75.37% examples, 435565 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:53,693 : INFO : EPOCH 5 - PROGRESS: at 85.49% examples, 438991 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:54,708 : INFO : EPOCH 5 - PROGRESS: at 94.27% examples, 434414 words/s, in_qsize 9, out_qsize 0\n",
      "2019-06-13 23:19:55,275 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2019-06-13 23:19:55,285 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-06-13 23:19:55,311 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-06-13 23:19:55,314 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-06-13 23:19:55,316 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-06-13 23:19:55,317 : INFO : EPOCH - 5 : training on 5844691 raw words (4654231 effective words) took 10.7s, 434033 effective words/s\n",
      "2019-06-13 23:19:55,319 : INFO : training on a 29223455 raw words (23270068 effective words) took 69.8s, 333178 effective words/s\n",
      "2019-06-13 23:19:55,320 : INFO : Saving trained Doc2Vec model\n",
      "2019-06-13 23:19:55,321 : INFO : saving Doc2Vec object under d2v.model, separately None\n",
      "2019-06-13 23:19:55,323 : INFO : storing np array 'syn1neg' to d2v.model.trainables.syn1neg.npy\n",
      "2019-06-13 23:19:55,662 : INFO : storing np array 'vectors' to d2v.model.wv.vectors.npy\n",
      "2019-06-13 23:19:55,953 : WARNING : this function is deprecated, use smart_open.open instead\n",
      "2019-06-13 23:19:56,784 : INFO : saved d2v.model\n"
     ]
    }
   ],
   "source": [
    "# 构建doc2vc的训练语料\n",
    "corpus=[]\n",
    "for index,text in enumerate(dataset.review):\n",
    "    label=str(index)\n",
    "    corpus.append(doc2vec.TaggedDocument(text.split(),[label]))\n",
    "corpus  \n",
    "    \n",
    "# 训练doc2vec\n",
    "logging.info(\"Building Doc2Vec vocabulary\")\n",
    "d2v=doc2vec.Doc2Vec(min_count=1,window=5,vector_size=100,\n",
    "                    workers=5,alpha=0.025,min_alpha=0.00025,dm=1)\n",
    "\n",
    "d2v.build_vocab(corpus)\n",
    "logging.info(\"Training Doc2Vec model\")\n",
    "d2v.train(corpus, total_examples=d2v.corpus_count, epochs=d2v.epochs)\n",
    "logging.info(\"Saving trained Doc2Vec model\")\n",
    "d2v.save(\"d2v.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I dont know why people think this is such a bad movie. Its got a pretty good plot, some good action, and the change of location for Harry does not hurt either. Sure some of its offensive and gratuitous but this is not the only movie like that. Eastwood is in good form as Dirty Harry, and I liked Pat Hingle in this movie as the small town cop. If you liked DIRTY HARRY, then you should see this one, its a lot better than THE DEAD POOL. 4/5'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 输出doc2vec\n",
    "dataset.review[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1.1383786 , -0.46784815,  0.06375925,  0.28962016, -0.24800839,\n",
       "        -0.06971131,  0.59335   ,  0.4376741 , -0.19636081, -0.58239317,\n",
       "         0.45597535, -0.18986915, -0.33574727, -0.22845191,  0.07775857,\n",
       "        -0.2017226 , -0.0140759 , -0.3009961 ,  0.09014519,  0.41243365,\n",
       "        -0.3334271 , -0.18992965, -0.7009654 ,  0.09367395, -0.31657284,\n",
       "        -0.0486575 ,  0.58614874,  0.06907474, -0.2246884 , -0.2485166 ,\n",
       "         0.47919285, -0.49073938,  0.194364  ,  0.37193924,  0.5740015 ,\n",
       "        -0.04464248,  0.1534153 , -0.02575611, -0.54227084,  0.9011591 ,\n",
       "        -0.57240605, -0.15567659,  0.17084531, -0.27343234, -0.40673628,\n",
       "        -0.2340103 ,  0.08737222, -0.0491593 ,  0.1989162 ,  0.00204547,\n",
       "         0.12318104, -0.8986703 , -0.11936127, -0.42431563,  0.01699058,\n",
       "         0.41401187, -0.37953588, -0.3621248 , -0.7772428 ,  0.12070463,\n",
       "         0.4678501 ,  0.10152269,  0.10237753, -0.34688845, -0.5331601 ,\n",
       "         0.8923948 , -0.30411497, -0.29061875, -0.07892439, -0.02689619,\n",
       "         0.29292715,  0.15312251, -0.44352925,  0.32691377,  0.27942613,\n",
       "         0.28943992, -0.25180525,  0.41585958,  0.04876891,  0.30957645,\n",
       "        -0.1320198 ,  0.6006674 , -0.28031453, -0.2637833 , -0.62584877,\n",
       "        -0.11302038, -0.8322833 ,  0.75634944, -0.40339538, -0.22296654,\n",
       "        -0.4153951 , -0.35705873,  0.27872732, -0.45283598, -0.02978593,\n",
       "         0.291504  , -0.05996024, -0.18948597,  0.3489236 ,  0.00495681],\n",
       "       dtype=float32),\n",
       " array([ 1.182108  , -0.42166638,  0.07258683,  0.29878747, -0.383954  ,\n",
       "        -0.15755406,  0.59373766,  0.5774807 , -0.26658058, -0.5560269 ,\n",
       "         0.45817947, -0.26136234, -0.19571789, -0.22199148, -0.0197012 ,\n",
       "        -0.20247243, -0.01385438, -0.35140073,  0.1344745 ,  0.3929749 ,\n",
       "        -0.29878306, -0.25721157, -0.88068277,  0.12794185, -0.3896562 ,\n",
       "        -0.14308445,  0.63065124,  0.1404208 , -0.24266893, -0.37742418,\n",
       "         0.6348625 , -0.4819352 ,  0.23958181,  0.40135   ,  0.5869124 ,\n",
       "         0.02464969,  0.10747334, -0.07049213, -0.5347573 ,  0.95434654,\n",
       "        -0.65917677, -0.14404193,  0.2152817 , -0.38011995, -0.361324  ,\n",
       "        -0.26674286, -0.00887777, -0.18569042,  0.2151359 ,  0.07309473,\n",
       "         0.14290746, -0.94774204, -0.10818829, -0.5382148 , -0.06032931,\n",
       "         0.40681547, -0.5070796 , -0.3316246 , -0.8174218 ,  0.09369648,\n",
       "         0.5133343 ,  0.02827627,  0.14439577, -0.37963724, -0.53137916,\n",
       "         0.9557996 , -0.34481338, -0.35008693, -0.04106677, -0.03187301,\n",
       "         0.28519955,  0.13311295, -0.32452258,  0.2627888 ,  0.28345788,\n",
       "         0.33639276, -0.15695229,  0.42347655,  0.02909614,  0.20718005,\n",
       "        -0.14821623,  0.6626246 , -0.23849426, -0.31329224, -0.56897825,\n",
       "        -0.18544048, -0.7488689 ,  0.8775361 , -0.40475965, -0.23444772,\n",
       "        -0.43698618, -0.2953417 ,  0.2723984 , -0.50057155,  0.04634719,\n",
       "         0.31388748, -0.15190265, -0.23404941,  0.463091  ,  0.0605871 ],\n",
       "       dtype=float32))"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1='I dont know why people think this is such a bad movie. '\n",
    "inferred_vector1 = d2v.infer_vector(x1.split(),steps=100, alpha=0.025)\n",
    "\n",
    "x2='I dont know why people think this is such a bad movie. '\n",
    "inferred_vector2 = d2v.infer_vector(x1.split(),steps=100, alpha=0.025)\n",
    "inferred_vector1,inferred_vector2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0119474694699786"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.cluster.util import cosine_distance\n",
    "cosine_distance(inferred_vector1,inferred_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://images2018.cnblogs.com/blog/1182656/201803/1182656-20180301160937850-1829406764.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9880526"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cosine_similarity(vector1,vector2):\n",
    "    return np.dot(vector1,vector2)/(np.linalg.norm(vector1)*(np.linalg.norm(vector2)))\n",
    "cosine_similarity(inferred_vector1, inferred_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 基于曼哈顿空间距离计算两个字符串语义空间表示相似度计算\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "感觉不靠谱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exponent_neg_manhattan_distance(sent_left, sent_right):\n",
    "    '''基于曼哈顿空间距离计算两个字符串语义空间表示相似度计算'''\n",
    "    return np.exp(-np.sum(np.abs(sent_left - sent_right)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0049027507"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "exponent_neg_manhattan_distance(inferred_vector1, inferred_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 n元语法相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(None, 'Chief'), ('Chief', 'Executive'), ('Executive', 'Officer'), ('Officer', None)]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "#这里展示2元语法\n",
    "text1 = 'Chief Executive Officer'\n",
    "\n",
    "#bigram考虑匹配开头和结束，所有使用pad_right和pad_left\n",
    "ceo_bigrams = nltk.bigrams(text1.split(),pad_right=True,pad_left=True)\n",
    "print(list(ceo_bigrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "#这里展示2元语法\n",
    "def bigram_distance(text1, text2):\n",
    "    #bigram考虑匹配开头和结束，所以使用pad_right和pad_left\n",
    "    text1_bigrams = nltk.bigrams(text1.split(),pad_right=True,pad_left=True)\n",
    "    text2_bigrams = nltk.bigrams(text2.split(), pad_right=True, pad_left=True)\n",
    "    #交集的长度\n",
    "    distance = len(set(text1_bigrams).intersection(set(text2_bigrams)))\n",
    "    return distance\n",
    "text1 = 'Chief Executive Officer is manager'\n",
    "text2 = 'Chief Technology Officer is technology manager'\n",
    "print(bigram_distance(text1, text2)) #相似度为3\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 jaccard相似性\n",
    "jaccard距离度量的两个集合的相似度，它是由 （集合1交集合2）/（结合1交结合2）计算而来的。\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import jaccard_distance\n",
    "\n",
    "#这里我们以单个的字符代表文本\n",
    "set1 = set(['a','b','c','d','a'])\n",
    "set2 = set(['a','b','e','g','a'])\n",
    "\n",
    "print(jaccard_distance(set1, set2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 masi距离\n",
    "\n",
    "masi距离度量是jaccard相似度的加权版本，当集合之间存在部分重叠时，通过调整得分来生成小于jaccard距离值。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6666666666666666\n",
      "0.89\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import jaccard_distance,masi_distance\n",
    "\n",
    "#这里我们以单个的字符代表文本\n",
    "set1 = set(['a','b','c','d','a'])\n",
    "set2 = set(['a','b','e','g','a'])\n",
    "\n",
    "print(jaccard_distance(set1, set2))\n",
    "print(masi_distance(set1, set2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 编辑距离\n",
    "\n",
    "编辑距离，又称为Levenshtein距离，是用于计算一个字符串转换为另一个字符串时，插入、删除和替换的次数。例如，将'dad'转换为'bad'需要一次替换操作，编辑距离为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "from nltk.metrics.distance import edit_distance\n",
    "\n",
    "str1 = 'bad'\n",
    "str2 = 'dad'\n",
    "print(edit_distance(str1, str2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "\n",
    "数据集来自：https://github.com/ibrahimsharaf/doc2vec\n",
    "\n",
    "https://github.com/iamxiaomu/doc2vec/blob/master/doc2vct.py\n",
    "\n",
    "https://www.cnblogs.com/bymo/p/8489037.html\n",
    "\n",
    "https://blog.csdn.net/u011897301/article/details/79083154\n",
    "\n",
    "https://www.jb51.net/article/136217.htm"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
