{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reference:https://github.com/puke3615/TextByRNN/blob/master/client.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sys\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风。'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data='去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风。'\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数设置\n",
    "# 开始结束标识位\n",
    "START,END='S','E'\n",
    "# 模型参数保存路径\n",
    "MODEL_PATH='data/'\n",
    "# 最大迭代次数\n",
    "MAX_ITERATOR=1000\n",
    "# 是否输出数据信息\n",
    "OUTPUT_DATA_INFO=True\n",
    "# 每步预测输出\n",
    "OUTPUT_EVERY_STEP=False\n",
    "# 训练\n",
    "TRAIN=True\n",
    "# 预测\n",
    "PREDICT=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参数初始化方法定义\n",
    "def init_weight(shape):\n",
    "    return tf.Variable(tf.truncated_normal(shape,stddev=0.1))\n",
    "def init_bias(shape):\n",
    "    return tf.Variable(tf.constant(0.01,shape=shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 这里涉及一个知识点，关于截断分布的好处：\n",
    " \n",
    " http://sofasofa.io/forum_main_post.php?postid=1001430\n",
    " \n",
    " https://blog.csdn.net/u013713117/article/details/65446361\n",
    " \n",
    " ![](data:image/png;base64,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)\n",
    " \n",
    " 在tf.truncated_normal中如果x的取值在区间（μ-2σ，μ+2σ）之外则重新进行选择。这样保证了生成的值都在均值附近。\n",
    " \n",
    " truncated_normal本质上也是生成正态分布的随机数，但是只保留两个标准差之内的点，在两个标准差之外的点就被舍去，然后重新抽样，直到达到你要求的个数为止。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型定义\n",
    "def model(x,batch_size):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    n_hidden=32\n",
    "    depth=int(x.get_shape()[-1])\n",
    "    W=init_weight([depth,n_hidden])\n",
    "    b=init_bias([n_hidden])\n",
    "    x=tf.matmul(x,W)+b\n",
    "    \n",
    "    x=tf.reshape(x,[batch_size,-1,n_hidden])\n",
    "    cell=tf.contrib.rnn.BasicLSTMCell(n_hidden)\n",
    "#     cell=tf.keras.layers.LSTMCell(n_hidden)\n",
    "\n",
    "    \n",
    "    initial_state=cell.zero_state(batch_size,tf.float32)\n",
    "    outputs,last_state=tf.nn.dynamic_rnn(cell,x,initial_state=initial_state)\n",
    "#     outputs,last_state=tf.keras.layers.RNN(cell,x,initial_state=initial_state)\n",
    "\n",
    "#     keras.layers.RNN(cell)\n",
    "    x=tf.reshape(outputs,[-1,n_hidden])\n",
    "    \n",
    "    W = init_weight([n_hidden, depth])\n",
    "    b = init_bias([depth])\n",
    "    x = tf.matmul(x, W) + b\n",
    "\n",
    "    return x, initial_state, last_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练\n",
    "def train(logits,labels,learning_rate=1e-2):\n",
    "    cross_entropy=tf.nn.softmax_cross_entropy_with_logits(labels=labels,logits=logits)\n",
    "    loss=tf.reduce_mean(cross_entropy)\n",
    "    optimizer=tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "    return loss,optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_info(array):\n",
    "    if not OUTPUT_DATA_INFO:\n",
    "        return\n",
    "    r = ''\n",
    "    if isinstance(array, dict):\n",
    "        array = ['%s:%s ' % (k, v) for k, v in array.items()]\n",
    "    for a in array:\n",
    "        if isinstance(a, int):\n",
    "            a = str(a) + ' '\n",
    "        if isinstance(a, list):\n",
    "            a = str(a) + ' '\n",
    "        r += a\n",
    "    print('[%s] %s' % (len(array), r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2onehot(w, words2index, depth):\n",
    "    index = words2index[w]\n",
    "    return [1 if index == i else 0 for i in range(depth)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[27] 笑日去此不依。年今面风相红桃何，旧E门S人映处春花中知\n",
      "[27] 0:笑 1:日 2:去 3:此 4:不 5:依 6:。 7:年 8:今 9:面 10:风 11:相 12:红 13:桃 14:何 15:， 16:旧 17:E 18:门 19:S 20:人 21:映 22:处 23:春 24:花 25:中 26:知 \n",
      "[27] 笑:0 日:1 去:2 此:3 不:4 依:5 。:6 年:7 今:8 面:9 风:10 相:11 红:12 桃:13 何:14 ，:15 旧:16 E:17 门:18 S:19 人:20 映:21 处:22 春:23 花:24 中:25 知:26 \n"
     ]
    }
   ],
   "source": [
    "data = START + data + END\n",
    "\n",
    "words = list(set(data))\n",
    "depth = len(words)\n",
    "index2words = {i: word for i, word in enumerate(words)}\n",
    "words2index = {v: k for k, v in index2words.items()}\n",
    "print_info(words)\n",
    "print_info(index2words)\n",
    "print_info(words2index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34] S去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风。E\n",
      "[34] 去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风。EE\n"
     ]
    }
   ],
   "source": [
    "x_data = data\n",
    "y_data = [x_data[i + 1] if i < len(x_data) - 1 else END for i in range(len(x_data))]\n",
    "print_info(x_data)\n",
    "print_info(y_data)\n",
    "x_data = list(map(lambda w: word2onehot(w, words2index, depth), x_data))\n",
    "y_data = list(map(lambda w: word2onehot(w, words2index, depth), y_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0,\n",
       "  0,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 27)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data[0],len(y_data[0]) \n",
    "# 因为y_data的第一个字为去，在word2index中去的id为10，所以one中索引为11处为1，其余位置为0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-5-9fb6c2a74d73>:12: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:From <ipython-input-5-9fb6c2a74d73>:17: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-6-7668e3377a3d>:3: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n",
      "WARNING:tensorflow:From D:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from data/model\n",
      "Load last model params successfully.\n",
      "\n",
      "Start training...\n",
      "Step 10, Loss 0.9236127\n",
      "Step 20, Loss 0.1344424\n",
      "Step 30, Loss 0.05505969\n",
      "Step 40, Loss 0.03505858\n",
      "Step 50, Loss 0.026216118\n",
      "Step 60, Loss 0.02096767\n",
      "Step 70, Loss 0.01720435\n",
      "Step 80, Loss 0.014521039\n",
      "Step 90, Loss 0.012488667\n",
      "Step 100, Loss 0.0109867\n",
      "Step 110, Loss 0.009825848\n",
      "Step 120, Loss 0.008896917\n",
      "Step 130, Loss 0.008131042\n",
      "Step 140, Loss 0.007490352\n",
      "Step 150, Loss 0.006948561\n",
      "Step 160, Loss 0.006483353\n",
      "Step 170, Loss 0.006078142\n",
      "Step 180, Loss 0.005720652\n",
      "Step 190, Loss 0.005401861\n",
      "Step 200, Loss 0.005114746\n",
      "Step 210, Loss 0.0048536165\n",
      "Step 220, Loss 0.004614016\n",
      "Step 230, Loss 0.00439195\n",
      "Step 240, Loss 0.004184311\n",
      "Step 250, Loss 0.0039892104\n",
      "Step 260, Loss 0.0038061384\n",
      "Step 270, Loss 0.0036348302\n",
      "Step 280, Loss 0.0034744497\n",
      "Step 290, Loss 0.003322592\n",
      "Step 300, Loss 0.0031773213\n",
      "Step 310, Loss 0.0030391044\n",
      "Step 320, Loss 0.0029111523\n",
      "Step 330, Loss 0.0027968462\n",
      "Step 340, Loss 0.0026957945\n",
      "Step 350, Loss 0.0026053651\n",
      "Step 360, Loss 0.002522828\n",
      "Step 370, Loss 0.002446374\n",
      "Step 380, Loss 0.0023750605\n",
      "Step 390, Loss 0.002308079\n",
      "Step 400, Loss 0.002245046\n",
      "Step 410, Loss 0.0021854814\n",
      "Step 420, Loss 0.0021291194\n",
      "Step 430, Loss 0.0020757269\n",
      "Step 440, Loss 0.0020248755\n",
      "Step 450, Loss 0.0019765527\n",
      "Step 460, Loss 0.0019303891\n",
      "Step 470, Loss 0.0018863367\n",
      "Step 480, Loss 0.001844197\n",
      "Step 490, Loss 0.0018038033\n",
      "Step 500, Loss 0.0017650507\n",
      "Step 510, Loss 0.0017278147\n",
      "Step 520, Loss 0.0016920494\n",
      "Step 530, Loss 0.0016576234\n",
      "Step 540, Loss 0.0016244279\n",
      "Step 550, Loss 0.0015925057\n",
      "Step 560, Loss 0.0015616156\n",
      "Step 570, Loss 0.0015318559\n",
      "Step 580, Loss 0.001503087\n",
      "Step 590, Loss 0.0014752669\n",
      "Step 600, Loss 0.0014483156\n",
      "Step 610, Loss 0.0014222824\n",
      "Step 620, Loss 0.0013970414\n",
      "Step 630, Loss 0.0013725611\n",
      "Step 640, Loss 0.0013488384\n",
      "Step 650, Loss 0.001325793\n",
      "Step 660, Loss 0.0013034005\n",
      "Step 670, Loss 0.0012816117\n",
      "Step 680, Loss 0.0012604658\n",
      "Step 690, Loss 0.0012398188\n",
      "Step 700, Loss 0.0012197063\n",
      "Step 710, Loss 0.0012000931\n",
      "Step 720, Loss 0.0011809126\n",
      "Step 730, Loss 0.0011621794\n",
      "Step 740, Loss 0.0011437951\n",
      "Step 750, Loss 0.0011258405\n",
      "Step 760, Loss 0.0011082875\n",
      "Step 770, Loss 0.0010910416\n",
      "Step 780, Loss 0.0010741728\n",
      "Step 790, Loss 0.0010576604\n",
      "Step 800, Loss 0.0010415006\n",
      "Step 810, Loss 0.0010257498\n",
      "Step 820, Loss 0.0010103481\n",
      "Step 830, Loss 0.000995422\n",
      "Step 840, Loss 0.0009808663\n",
      "Step 850, Loss 0.00096675457\n",
      "Step 860, Loss 0.00095303106\n",
      "Step 870, Loss 0.00093968835\n",
      "Step 880, Loss 0.00092669553\n",
      "Step 890, Loss 0.0009140455\n",
      "Step 900, Loss 0.000901738\n",
      "Step 910, Loss 0.00088977686\n",
      "Step 920, Loss 0.0008780989\n",
      "Step 930, Loss 0.0008666939\n",
      "Step 940, Loss 0.000855558\n",
      "Step 950, Loss 0.0008446775\n",
      "Step 960, Loss 0.00083408045\n",
      "Step 970, Loss 0.00082370715\n",
      "Step 980, Loss 0.0008135402\n",
      "Step 990, Loss 0.00080356555\n",
      "Step 1000, Loss 0.0007938289\n",
      "\n",
      "String predict...\n",
      "\n",
      "outputs: [[1.2688788e-05 8.3024184e-05 9.9889427e-01 2.7153700e-07 1.2066010e-05\n",
      "  3.0661347e-05 6.2529230e-05 4.1204639e-06 9.1696900e-05 4.6344041e-05\n",
      "  5.9499748e-06 1.1811769e-04 2.4278106e-05 1.2742606e-05 4.2071019e-06\n",
      "  3.1057381e-05 3.3733922e-05 1.0271442e-05 3.3676415e-07 1.7140493e-07\n",
      "  2.9319022e-05 3.2672440e-07 8.6196565e-07 3.4783660e-08 4.3325705e-04\n",
      "  5.6869208e-05 1.0804546e-06]]\n",
      "去年今日此门中，人面桃花相映红，人面不知何处去，桃花依旧笑春风。\n"
     ]
    }
   ],
   "source": [
    "x = tf.placeholder(tf.float32, [None, depth])\n",
    "y = tf.placeholder(tf.float32, [None, depth])\n",
    "\n",
    "logits, initial_state, last_state = model(x, 1)\n",
    "loss, train_step = train(logits, y)\n",
    "\n",
    "if not TRAIN and not PREDICT:\n",
    "    exit()\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "checkpoint = tf.train.latest_checkpoint(MODEL_PATH)\n",
    "if checkpoint:\n",
    "    saver.restore(sess, checkpoint)\n",
    "    print('Load last model params successfully.') \n",
    "\n",
    "if TRAIN:\n",
    "    print('\\nStart training...')\n",
    "    for step in range(1, MAX_ITERATOR + 1):\n",
    "        loss_value, _ = sess.run([loss, train_step], feed_dict={x: x_data, y: y_data})\n",
    "        if step % 10 == 0 or step == MAX_ITERATOR:\n",
    "            print('Step %s, Loss %s' % (step, loss_value)) \n",
    "        if step % 50 == 0 or step == MAX_ITERATOR:\n",
    "            saver.save(sess, os.path.join(MODEL_PATH, 'model'))\n",
    "if PREDICT:\n",
    "    print('\\nString predict...') \n",
    "    inputs = [START]\n",
    "    inputs = list(map(lambda w: word2onehot(w, words2index, depth), inputs))\n",
    "    predict = tf.nn.softmax(logits)\n",
    "    outputs, last_state_value = sess.run([predict, last_state], feed_dict={x: inputs})\n",
    "    print(\"\\noutputs:\",outputs)\n",
    "    word = index2words[np.argmax(outputs)]\n",
    "    result = ''\n",
    "    while word != END:\n",
    "        result += word\n",
    "        if OUTPUT_EVERY_STEP:\n",
    "            sys.stdout.write(word)\n",
    "            sys.stdout.flush()\n",
    "        outputs, last_state_value = sess.run(\n",
    "            [predict, last_state], feed_dict={x: outputs, initial_state: last_state_value})\n",
    "        word = index2words[np.argmax(outputs)]\n",
    "    print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
